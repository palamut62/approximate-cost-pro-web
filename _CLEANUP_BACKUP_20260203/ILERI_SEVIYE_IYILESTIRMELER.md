# ğŸš€ Ä°leri Seviye AI Analiz Sistemi Ä°yileÅŸtirmeleri

**Tarih:** 2026-02-02  
**Kapsam:** Temel iyileÅŸtirmelerin Ã¶tesinde, sistemi profesyonel seviyeye taÅŸÄ±yacak stratejiler

---

## ğŸ“Š BÃ–LÃœM 1: VERÄ° KALÄ°TESÄ° VE ZENGÄ°NLEÅTÄ°RME

### 1.1 Resmi Kaynaklardan Veri Ã‡ekme

**AmaÃ§:** Ã‡ÅB, KÄ°K, DSÄ° resmi birim fiyat analizlerini otomatik Ã§ek

```python
# backend/services/official_data_scraper.py

import requests
from bs4 import BeautifulSoup
import pdfplumber
from typing import List, Dict
import json

class OfficialDataScraper:
    """Resmi kurumlardan birim fiyat verisi Ã§ekme"""
    
    def __init__(self):
        self.sources = {
            'csb': 'https://webdosya.csb.gov.tr/db/birimfiyat/',
            'kik': 'https://ekap.kik.gov.tr/',
            'dsi': 'https://www.dsi.gov.tr/birimfiyatlar'
        }
        
    async def fetch_csb_unit_prices(self, year: int = 2025) -> List[Dict]:
        """Ã‡ÅB birim fiyat analizlerini Ã§ek"""
        
        # PDF'leri indir ve parse et
        analyses = []
        
        # Ã–rnek: Ä°nÅŸaat iÅŸleri birim fiyat analizi
        pdf_url = f"{self.sources['csb']}{year}/insaat_birim_fiyat.pdf"
        
        try:
            response = requests.get(pdf_url, timeout=30)
            
            with pdfplumber.open(io.BytesIO(response.content)) as pdf:
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        parsed = self._parse_analysis_table(table)
                        if parsed:
                            analyses.extend(parsed)
                            
        except Exception as e:
            print(f"Ã‡ÅB veri Ã§ekme hatasÄ±: {e}")
            
        return analyses
    
    def _parse_analysis_table(self, table: list) -> List[Dict]:
        """Analiz tablosunu parse et"""
        results = []
        
        current_poz = None
        components = []
        
        for row in table:
            if not row or not row[0]:
                continue
                
            # Poz numarasÄ± satÄ±rÄ± mÄ±?
            if self._is_poz_header(row):
                if current_poz and components:
                    results.append({
                        'poz_no': current_poz['code'],
                        'description': current_poz['desc'],
                        'unit': current_poz['unit'],
                        'components': components
                    })
                current_poz = self._extract_poz_info(row)
                components = []
            else:
                # BileÅŸen satÄ±rÄ±
                comp = self._extract_component(row)
                if comp:
                    components.append(comp)
                    
        return results


class AnalysisDataEnricher:
    """Mevcut verileri zenginleÅŸtir"""
    
    def __init__(self, db_manager):
        self.db = db_manager
        self.scraper = OfficialDataScraper()
        
    async def enrich_from_official_sources(self):
        """Resmi kaynaklardan veri Ã§ek ve veritabanÄ±nÄ± gÃ¼ncelle"""
        
        # 1. Ã‡ÅB verilerini Ã§ek
        csb_data = await self.scraper.fetch_csb_unit_prices(2025)
        
        # 2. Mevcut verilerle karÅŸÄ±laÅŸtÄ±r ve gÃ¼ncelle
        for analysis in csb_data:
            existing = self.db.get_analysis_by_poz(analysis['poz_no'])
            
            if existing:
                # GÃ¼ncelle
                self.db.update_analysis(analysis['poz_no'], {
                    'components': analysis['components'],
                    'source': 'csb_official',
                    'last_updated': datetime.now()
                })
            else:
                # Yeni ekle
                self.db.insert_analysis(analysis)
                
        print(f"âœ… {len(csb_data)} analiz gÃ¼ncellendi/eklendi")
```

---

### 1.2 Aktif Ã–ÄŸrenme (Active Learning)

**AmaÃ§:** AI'Ä±n en Ã§ok emin olmadÄ±ÄŸÄ± Ã¶rnekleri kullanÄ±cÄ±ya sor

```python
# backend/services/active_learning_service.py

from typing import List, Dict, Tuple
import numpy as np

class ActiveLearningService:
    """
    Belirsizlik tabanlÄ± aktif Ã¶ÄŸrenme.
    AI'Ä±n dÃ¼ÅŸÃ¼k gÃ¼venle Ã¼rettiÄŸi sonuÃ§larÄ± iÅŸaretle ve kullanÄ±cÄ±dan doÄŸrulama iste.
    """
    
    def __init__(self, ai_service, db_manager):
        self.ai_service = ai_service
        self.db = db_manager
        self.uncertainty_threshold = 0.6
        
    async def analyze_with_uncertainty(self, description: str, unit: str) -> Dict:
        """Belirsizlik skoru ile analiz yap"""
        
        # Birden fazla sonuÃ§ al (farklÄ± temperature)
        results = []
        for temp in [0.1, 0.3, 0.5]:
            result = await self.ai_service.generate_analysis(
                description, unit, temperature=temp
            )
            results.append(result)
        
        # Belirsizlik hesapla
        uncertainty = self._calculate_uncertainty(results)
        
        # En iyi sonucu seÃ§
        best_result = self._select_best(results)
        best_result['uncertainty'] = uncertainty
        best_result['needs_review'] = uncertainty > self.uncertainty_threshold
        
        # YÃ¼ksek belirsizlikli sonuÃ§larÄ± kaydet (kullanÄ±cÄ± doÄŸrulamasÄ± iÃ§in)
        if uncertainty > self.uncertainty_threshold:
            self._queue_for_review(description, unit, best_result, uncertainty)
            
        return best_result
    
    def _calculate_uncertainty(self, results: List[Dict]) -> float:
        """SonuÃ§lar arasÄ± uyumsuzluktan belirsizlik hesapla"""
        
        if len(results) < 2:
            return 0.5
        
        # 1. Component sayÄ±sÄ± varyansÄ±
        counts = [len(r.get('components', [])) for r in results]
        count_variance = np.var(counts) / (np.mean(counts) + 1)
        
        # 2. Toplam fiyat varyansÄ±
        prices = []
        for r in results:
            total = sum(c.get('total_price', 0) for c in r.get('components', []))
            prices.append(total)
        
        price_cv = np.std(prices) / (np.mean(prices) + 1)  # Coefficient of variation
        
        # 3. Component isim uyumsuzluÄŸu
        all_names = []
        for r in results:
            names = set()
            for c in r.get('components', []):
                # Ä°smin ilk 3 kelimesini al
                name_key = ' '.join(c.get('name', '').lower().split()[:3])
                names.add(name_key)
            all_names.append(names)
        
        # Jaccard distance
        if len(all_names) >= 2:
            intersection = all_names[0]
            union = all_names[0]
            for names in all_names[1:]:
                intersection = intersection & names
                union = union | names
            
            jaccard_sim = len(intersection) / max(len(union), 1)
            name_uncertainty = 1 - jaccard_sim
        else:
            name_uncertainty = 0.5
        
        # Toplam belirsizlik
        uncertainty = (count_variance * 0.3 + price_cv * 0.3 + name_uncertainty * 0.4)
        
        return min(1.0, uncertainty)
    
    def _queue_for_review(self, description: str, unit: str, result: Dict, uncertainty: float):
        """KullanÄ±cÄ± incelemesi iÃ§in kuyruÄŸa ekle"""
        
        self.db.insert_review_queue({
            'description': description,
            'unit': unit,
            'ai_result': result,
            'uncertainty': uncertainty,
            'status': 'pending',
            'created_at': datetime.now()
        })
    
    def get_pending_reviews(self, limit: int = 10) -> List[Dict]:
        """Bekleyen incelemeleri getir"""
        return self.db.get_pending_reviews(limit)
    
    def submit_review(self, review_id: int, corrected_result: Dict, user_notes: str):
        """KullanÄ±cÄ± incelemesini kaydet"""
        
        # 1. Ä°ncelemeyi tamamla
        self.db.complete_review(review_id, corrected_result, user_notes)
        
        # 2. EÄŸitim verisine ekle
        original = self.db.get_review(review_id)
        self.db.add_training_example({
            'input': original['description'],
            'output': corrected_result,
            'source': 'active_learning',
            'confidence': 1.0  # KullanÄ±cÄ± doÄŸrulamasÄ±
        })
        
        print(f"âœ… Ä°nceleme #{review_id} kaydedildi ve eÄŸitim verisine eklendi")
```

---

### 1.3 Sentetik Veri Ãœretimi

**AmaÃ§:** Mevcut verileri Ã§eÅŸitlendirerek eÄŸitim setini geniÅŸlet

```python
# backend/services/synthetic_data_generator.py

import random
from typing import List, Dict

class SyntheticDataGenerator:
    """Sentetik eÄŸitim verisi Ã¼retimi"""
    
    def __init__(self):
        # Varyasyon ÅŸablonlarÄ±
        self.concrete_classes = ['C20/25', 'C25/30', 'C30/37', 'C35/45', 'C40/50']
        self.rebar_diameters = ['Ã˜8', 'Ã˜10', 'Ã˜12', 'Ã˜14', 'Ã˜16', 'Ã˜18', 'Ã˜20']
        self.steel_grades = ['S420', 'S500']
        self.wall_thicknesses = ['10 cm', '15 cm', '20 cm', '25 cm']
        self.brick_types = ['tuÄŸla', 'briket', 'gazbeton', 'bims blok']
        
        # Miktar aralÄ±klarÄ±
        self.quantity_ranges = {
            'mÂ²': (5, 500),
            'mÂ³': (1, 100),
            'm': (10, 1000),
            'ton': (0.1, 50),
            'adet': (1, 1000)
        }
        
    def generate_variations(self, base_example: Dict, n_variations: int = 5) -> List[Dict]:
        """Bir Ã¶rnekten varyasyonlar Ã¼ret"""
        
        variations = []
        description = base_example.get('input', '')
        
        for _ in range(n_variations):
            new_desc = description
            new_output = base_example.get('output', {}).copy()
            
            # Beton sÄ±nÄ±fÄ± deÄŸiÅŸtir
            for old_class in self.concrete_classes:
                if old_class in new_desc:
                    new_class = random.choice(self.concrete_classes)
                    new_desc = new_desc.replace(old_class, new_class)
                    # Output'taki beton sÄ±nÄ±fÄ±nÄ± da gÃ¼ncelle
                    self._update_concrete_class(new_output, new_class)
                    break
            
            # Demir Ã§apÄ± deÄŸiÅŸtir
            for old_dia in self.rebar_diameters:
                if old_dia in new_desc:
                    new_dia = random.choice(self.rebar_diameters)
                    new_desc = new_desc.replace(old_dia, new_dia)
                    break
            
            # Duvar kalÄ±nlÄ±ÄŸÄ± deÄŸiÅŸtir
            for old_thick in self.wall_thicknesses:
                if old_thick in new_desc:
                    new_thick = random.choice(self.wall_thicknesses)
                    new_desc = new_desc.replace(old_thick, new_thick)
                    break
            
            # Miktar varyasyonu
            new_desc = self._vary_quantity(new_desc)
            
            variations.append({
                'instruction': base_example.get('instruction', ''),
                'input': new_desc,
                'output': new_output,
                'source': 'synthetic',
                'base_example_id': base_example.get('id')
            })
            
        return variations
    
    def _vary_quantity(self, description: str) -> str:
        """MiktarlarÄ± rastgele deÄŸiÅŸtir"""
        import re
        
        # Miktar kalÄ±plarÄ±nÄ± bul: "50 mÂ³", "100 mÂ²", vb.
        pattern = r'(\d+(?:[.,]\d+)?)\s*(m[Â²Â³2]|m\b|ton|kg|adet)'
        
        def replace_quantity(match):
            old_qty = float(match.group(1).replace(',', '.'))
            unit = match.group(2)
            
            # Â±30% varyasyon
            variation = random.uniform(0.7, 1.3)
            new_qty = old_qty * variation
            
            # Birime gÃ¶re yuvarla
            if unit in ['adet', 'kg']:
                new_qty = int(new_qty)
            else:
                new_qty = round(new_qty, 1)
                
            return f"{new_qty} {unit}"
        
        return re.sub(pattern, replace_quantity, description)
    
    def generate_edge_cases(self) -> List[Dict]:
        """Edge case Ã¶rnekleri Ã¼ret"""
        
        edge_cases = [
            # Belirsiz ifadeler
            {
                'input': 'beton dÃ¶ÅŸeme',  # C sÄ±nÄ±fÄ± yok
                'expected_behavior': 'C25/30 varsayÄ±lmalÄ±'
            },
            {
                'input': 'duvar',  # Tip belirtilmemiÅŸ
                'expected_behavior': 'En yaygÄ±n tip olan tuÄŸla varsayÄ±lmalÄ±'
            },
            
            # KarÄ±ÅŸÄ±k ifadeler
            {
                'input': 'betonarme kanal',  # Kanal normalde yalÄ±n beton
                'expected_behavior': 'betonarme dendiÄŸi iÃ§in demir eklemeli'
            },
            {
                'input': 'dÃ¼z beton temel',  # "dÃ¼z" = yalÄ±n
                'expected_behavior': 'Demir EKLEMEMELÄ°'
            },
            
            # Birim belirsizlikleri
            {
                'input': '100 beton kanal',  # Birim yok
                'expected_behavior': 'm (metre) varsayÄ±lmalÄ± (kanal iÃ§in)'
            },
            
            # Ã–lÃ§ek belirsizlikleri
            {
                'input': '0.5 mÂ³ beton',  # Ã‡ok kÃ¼Ã§Ã¼k miktar
                'expected_behavior': 'Minimum sipariÅŸ miktarÄ± uyarÄ±sÄ±'
            },
            {
                'input': '10000 mÂ³ beton',  # Ã‡ok bÃ¼yÃ¼k miktar
                'expected_behavior': 'BÃ¼yÃ¼k proje uyarÄ±sÄ±, fiyat indirimi Ã¶nerisi'
            }
        ]
        
        return edge_cases
```

---

## ğŸ“Š BÃ–LÃœM 2: MODEL Ä°YÄ°LEÅTÄ°RMELERÄ°

### 2.1 Fine-Tuning Pipeline

**AmaÃ§:** Kendi verilerinle Ã¶zelleÅŸtirilmiÅŸ model eÄŸit

```python
# scripts/finetune_pipeline.py

"""
OpenAI veya Anthropic fine-tuning pipeline.
TemizlenmiÅŸ eÄŸitim verileriyle model Ã¶zelleÅŸtirme.
"""

import json
import openai
from pathlib import Path

class FineTuningPipeline:
    """Model fine-tuning pipeline"""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        
    def prepare_training_data(self, input_file: str, output_file: str):
        """EÄŸitim verisini fine-tuning formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r"""
        
        with open(input_file, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        formatted = []
        for item in data:
            # OpenAI fine-tuning formatÄ±
            formatted.append({
                "messages": [
                    {
                        "role": "system",
                        "content": "Sen TÃ¼rkiye'de Ã§alÄ±ÅŸan uzman bir YaklaÅŸÄ±k Maliyet ve Ä°hale UzmanÄ± Ä°nÅŸaat MÃ¼hendisisin. Verilen imalat tanÄ±mÄ± iÃ§in Ã‡ÅB standartlarÄ±na uygun birim fiyat analizi oluÅŸturursun."
                    },
                    {
                        "role": "user",
                        "content": f"Åu imalat iÃ§in birim fiyat analizi oluÅŸtur: {item['input']}"
                    },
                    {
                        "role": "assistant",
                        "content": json.dumps(item['output'], ensure_ascii=False)
                    }
                ]
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in formatted:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                
        print(f"âœ… {len(formatted)} Ã¶rnek hazÄ±rlandÄ±: {output_file}")
        return output_file
    
    def validate_training_data(self, file_path: str) -> Dict:
        """EÄŸitim verisini doÄŸrula"""
        
        issues = []
        stats = {'total': 0, 'valid': 0, 'invalid': 0}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f, 1):
                stats['total'] += 1
                try:
                    item = json.loads(line)
                    
                    # Zorunlu alanlar
                    if 'messages' not in item:
                        issues.append(f"SatÄ±r {i}: 'messages' eksik")
                        stats['invalid'] += 1
                        continue
                    
                    # Mesaj yapÄ±sÄ±
                    messages = item['messages']
                    if len(messages) < 3:
                        issues.append(f"SatÄ±r {i}: En az 3 mesaj gerekli")
                        stats['invalid'] += 1
                        continue
                    
                    # Roller
                    roles = [m.get('role') for m in messages]
                    if roles != ['system', 'user', 'assistant']:
                        issues.append(f"SatÄ±r {i}: YanlÄ±ÅŸ rol sÄ±rasÄ±")
                        stats['invalid'] += 1
                        continue
                    
                    stats['valid'] += 1
                    
                except json.JSONDecodeError as e:
                    issues.append(f"SatÄ±r {i}: JSON parse hatasÄ± - {e}")
                    stats['invalid'] += 1
        
        return {
            'stats': stats,
            'issues': issues[:20],  # Ä°lk 20 hata
            'valid': stats['invalid'] == 0
        }
    
    def start_finetuning(self, training_file: str, model: str = "gpt-4o-mini-2024-07-18"):
        """Fine-tuning iÅŸini baÅŸlat"""
        
        # 1. DosyayÄ± yÃ¼kle
        print("ğŸ“¤ EÄŸitim dosyasÄ± yÃ¼kleniyor...")
        file_response = self.client.files.create(
            file=open(training_file, 'rb'),
            purpose='fine-tune'
        )
        
        # 2. Fine-tuning iÅŸini baÅŸlat
        print("ğŸš€ Fine-tuning baÅŸlatÄ±lÄ±yor...")
        job = self.client.fine_tuning.jobs.create(
            training_file=file_response.id,
            model=model,
            hyperparameters={
                "n_epochs": 3,
                "batch_size": 4,
                "learning_rate_multiplier": 1.0
            }
        )
        
        print(f"âœ… Fine-tuning job baÅŸlatÄ±ldÄ±: {job.id}")
        return job.id
    
    def check_status(self, job_id: str):
        """Fine-tuning durumunu kontrol et"""
        job = self.client.fine_tuning.jobs.retrieve(job_id)
        
        print(f"""
Fine-Tuning Durumu:
  Job ID: {job.id}
  Status: {job.status}
  Model: {job.fine_tuned_model or 'HenÃ¼z hazÄ±r deÄŸil'}
  Created: {job.created_at}
        """)
        
        return job


# KullanÄ±m
if __name__ == "__main__":
    pipeline = FineTuningPipeline(api_key="sk-...")
    
    # 1. Veri hazÄ±rla
    pipeline.prepare_training_data(
        'egitim_verisi_CLEANED.jsonl',
        'finetuning_ready.jsonl'
    )
    
    # 2. DoÄŸrula
    validation = pipeline.validate_training_data('finetuning_ready.jsonl')
    if not validation['valid']:
        print("âŒ Veri hatalÄ±:", validation['issues'])
    else:
        # 3. EÄŸitimi baÅŸlat
        job_id = pipeline.start_finetuning('finetuning_ready.jsonl')
```

---

### 2.2 RAG (Retrieval-Augmented Generation) Sistemi

**AmaÃ§:** Her sorgu iÃ§in en alakalÄ± referans analizleri dinamik olarak getir

```python
# backend/services/rag_service.py

from typing import List, Dict
import chromadb
from sentence_transformers import SentenceTransformer

class RAGService:
    """
    Retrieval-Augmented Generation sistemi.
    Her analiz iÃ§in en alakalÄ± referans pozlarÄ± ve analizleri getirir.
    """
    
    def __init__(self):
        # Embedding modeli (TÃ¼rkÃ§e destekli)
        self.embedder = SentenceTransformer('emrecan/bert-base-turkish-cased-mean-nli-stsb-tr')
        
        # ChromaDB (vektÃ¶r veritabanÄ±)
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # Koleksiyonlar
        self.poz_collection = self.chroma_client.get_or_create_collection(
            name="poz_analyses",
            metadata={"hnsw:space": "cosine"}
        )
        
        self.feedback_collection = self.chroma_client.get_or_create_collection(
            name="user_feedbacks",
            metadata={"hnsw:space": "cosine"}
        )
        
    def index_poz_data(self, poz_records: List[Dict]):
        """POZ verilerini indexle"""
        
        documents = []
        embeddings = []
        ids = []
        metadatas = []
        
        for poz in poz_records:
            doc_text = f"{poz['poz_no']} - {poz['description']}"
            documents.append(doc_text)
            ids.append(poz['poz_no'])
            metadatas.append({
                'unit': poz.get('unit', ''),
                'price': str(poz.get('unit_price', '')),
                'institution': poz.get('institution', '')
            })
        
        # Batch embedding
        embeddings = self.embedder.encode(documents, show_progress_bar=True).tolist()
        
        # ChromaDB'ye ekle
        self.poz_collection.add(
            documents=documents,
            embeddings=embeddings,
            ids=ids,
            metadatas=metadatas
        )
        
        print(f"âœ… {len(poz_records)} POZ indexlendi")
        
    def retrieve_relevant_context(self, query: str, n_results: int = 10) -> Dict:
        """
        Sorgu iÃ§in en alakalÄ± context'i getir.
        POZ verileri + kullanÄ±cÄ± feedback'leri birleÅŸtirilir.
        """
        
        # 1. POZ aramasÄ±
        poz_results = self.poz_collection.query(
            query_texts=[query],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        # 2. Feedback aramasÄ±
        feedback_results = self.feedback_collection.query(
            query_texts=[query],
            n_results=5,
            include=['documents', 'metadatas', 'distances']
        )
        
        # 3. Context oluÅŸtur
        context = self._build_context(query, poz_results, feedback_results)
        
        return context
    
    def _build_context(self, query: str, poz_results: Dict, feedback_results: Dict) -> Dict:
        """RAG context'i oluÅŸtur"""
        
        context_parts = []
        
        # POZ referanslarÄ±
        context_parts.append("â•â•â• BENZER POZ ANALÄ°ZLERÄ° â•â•â•")
        
        for i, (doc, meta, dist) in enumerate(zip(
            poz_results['documents'][0],
            poz_results['metadatas'][0],
            poz_results['distances'][0]
        )):
            similarity = 1 - dist  # Cosine distance -> similarity
            if similarity > 0.5:  # Sadece alakalÄ± olanlar
                context_parts.append(f"""
ğŸ“Œ Referans #{i+1} (Benzerlik: {similarity:.0%})
   {doc}
   Birim: {meta.get('unit', '-')}
   Fiyat: {meta.get('price', '-')} TL
   Kurum: {meta.get('institution', '-')}
""")
        
        # Feedback referanslarÄ±
        if feedback_results['documents'][0]:
            context_parts.append("\nâ•â•â• KULLANICI DÃœZELTMELERÄ° â•â•â•")
            
            for doc, meta, dist in zip(
                feedback_results['documents'][0],
                feedback_results['metadatas'][0],
                feedback_results['distances'][0]
            ):
                similarity = 1 - dist
                if similarity > 0.6:
                    context_parts.append(f"""
âš ï¸ Ã–nceki DÃ¼zeltme (Benzerlik: {similarity:.0%})
   {doc}
   DÃ¼zeltme: {meta.get('correction', '-')}
""")
        
        return {
            'context_text': '\n'.join(context_parts),
            'poz_count': len(poz_results['documents'][0]),
            'feedback_count': len(feedback_results['documents'][0]),
            'top_similarity': 1 - min(poz_results['distances'][0]) if poz_results['distances'][0] else 0
        }
    
    def augmented_prompt(self, description: str, unit: str, base_prompt: str) -> str:
        """RAG ile zenginleÅŸtirilmiÅŸ prompt oluÅŸtur"""
        
        context = self.retrieve_relevant_context(description)
        
        augmented = f"""{base_prompt}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                   RAG CONTEXT (Otomatik Getirilen)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{context['context_text']}

âš ï¸ YukarÄ±daki referanslarÄ± MUTLAKA dikkate al!
   - Benzer pozlarÄ±n miktar ve fiyatlarÄ±nÄ± referans al
   - KullanÄ±cÄ± dÃ¼zeltmelerindeki hatalarÄ± TEKRARLAMA

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        GÃ–REV
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AÅŸaÄŸÄ±daki imalat iÃ§in analiz oluÅŸtur:
ğŸ“Œ TANIM: {description}
ğŸ“Œ BÄ°RÄ°M: {unit}
"""
        
        return augmented
```

---

### 2.3 Chain-of-Thought (CoT) Prompting

**AmaÃ§:** AI'Ä±n adÄ±m adÄ±m dÃ¼ÅŸÃ¼nmesini saÄŸla

```python
# backend/services/cot_analysis_service.py

class ChainOfThoughtAnalysisService:
    """
    Chain-of-Thought ile adÄ±m adÄ±m analiz.
    AI Ã¶nce dÃ¼ÅŸÃ¼nÃ¼r, sonra analiz oluÅŸturur.
    """
    
    def build_cot_prompt(self, description: str, unit: str, context: str) -> str:
        """CoT prompt oluÅŸtur"""
        
        return f"""Sen 20+ yÄ±l deneyimli bir YaklaÅŸÄ±k Maliyet UzmanÄ± Ä°nÅŸaat MÃ¼hendisisin.

GÃ–REV: AÅŸaÄŸÄ±daki imalat iÃ§in birim fiyat analizi oluÅŸtur.

ğŸ“Œ TANIM: {description}
ğŸ“Œ BÄ°RÄ°M: {unit}

{context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    ADIM ADIM DÃœÅÃœN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Analiz oluÅŸturmadan Ã–NCE, aÅŸaÄŸÄ±daki sorularÄ± yanÄ±tla:

<thinking>
1. Ä°MALAT TÄ°PÄ° NEDÄ°R?
   - [ ] Betonarme mi? (Demir ZORUNLU)
   - [ ] YalÄ±n beton mu? (Demir YOK)
   - [ ] HazÄ±r beton mu? (Ã‡imento/kum/Ã§akÄ±l YOK)
   - [ ] Duvar mÄ±? (HarÃ§ ZORUNLU)
   - [ ] KazÄ± mÄ±?
   - [ ] Kaplama mÄ±?
   - [ ] DiÄŸer: ___

2. DOÄRU BÄ°RÄ°M NEDÄ°R?
   - Kanal, boru = m (metre)
   - Duvar, dÃ¶ÅŸeme, kaplama = mÂ²
   - KazÄ±, beton hacim = mÂ³
   - Prefabrik = adet
   
   â†’ Bu imalat iÃ§in doÄŸru birim: ___

3. ZORUNLU BÄ°LEÅENLER:
   - Bu imalat tipi iÃ§in MUTLAKA olmasÄ± gerekenler:
     â–¡ ___
     â–¡ ___
     â–¡ ___

4. YASAK BÄ°LEÅENLER:
   - Bu imalat iÃ§in OLMAMASI gerekenler:
     â›” ___
     â›” ___

5. MÄ°KTAR KONTROLÃœ:
   - Emsal pozlara gÃ¶re tipik miktarlar:
     - Malzeme X: ___ birim
     - Ä°ÅŸÃ§ilik Y: ___ saat
     - Nakliye: ___ ton/mÂ³

6. FÄ°YAT KONTROLÃœ:
   - VeritabanÄ±/piyasa fiyatlarÄ±:
     - Malzeme X: ___ TL
     - Ä°ÅŸÃ§ilik Y: ___ TL/saat
</thinking>

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                       ANALÄ°Z OLUÅTUR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

YukarÄ±daki dÃ¼ÅŸÃ¼nme sÃ¼recine UYGUN bir JSON analizi oluÅŸtur:

{{
  "thinking_summary": "KÄ±sa dÃ¼ÅŸÃ¼nce Ã¶zeti",
  "construction_type": "tespit edilen imalat tipi",
  "suggested_unit": "doÄŸru birim",
  "components": [
    // Sadece ZORUNLU ve UYGUN bileÅŸenler
  ],
  "explanation": "DetaylÄ± aÃ§Ä±klama"
}}

âš ï¸ Ã–NEMLÄ°: 
- "thinking" bÃ¶lÃ¼mÃ¼ndeki kararlarÄ±nla Ã‡ELÄ°ÅME!
- YalÄ±n beton dediysen demir EKLEME!
- HazÄ±r beton dediysen Ã§imento/kum/Ã§akÄ±l EKLEME!
"""
    
    def parse_cot_response(self, response: str) -> Dict:
        """CoT yanÄ±tÄ±nÄ± parse et"""
        
        result = {
            'thinking': None,
            'analysis': None
        }
        
        # Thinking bÃ¶lÃ¼mÃ¼nÃ¼ Ã§Ä±kar
        thinking_match = re.search(r'<thinking>(.*?)</thinking>', response, re.DOTALL)
        if thinking_match:
            result['thinking'] = thinking_match.group(1).strip()
        
        # JSON analizi Ã§Ä±kar
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            try:
                result['analysis'] = json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        return result
    
    def validate_consistency(self, thinking: str, analysis: Dict) -> List[str]:
        """DÃ¼ÅŸÃ¼nme ve analiz tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol et"""
        
        issues = []
        
        if not thinking or not analysis:
            return issues
        
        thinking_lower = thinking.lower()
        components = analysis.get('components', [])
        component_names = ' '.join(c.get('name', '').lower() for c in components)
        
        # YalÄ±n beton kontrolÃ¼
        if 'yalÄ±n beton' in thinking_lower or 'demir yok' in thinking_lower:
            if 'demir' in component_names or 'donatÄ±' in component_names:
                issues.append("âŒ TUTARSIZLIK: YalÄ±n beton denildi ama demir eklendi!")
        
        # HazÄ±r beton kontrolÃ¼
        if 'hazÄ±r beton' in thinking_lower:
            if 'Ã§imento' in component_names and 'kum' in component_names:
                issues.append("âŒ TUTARSIZLIK: HazÄ±r beton denildi ama Ã§imento/kum eklendi!")
        
        # Betonarme kontrolÃ¼
        if 'betonarme' in thinking_lower or 'demir zorunlu' in thinking_lower:
            if 'demir' not in component_names and 'donatÄ±' not in component_names:
                issues.append("âŒ TUTARSIZLIK: Betonarme denildi ama demir eklenmedi!")
        
        return issues
```

---

## ğŸ“Š BÃ–LÃœM 3: KALÄ°TE KONTROL VE Ä°ZLEME

### 3.1 Otomatik Regression Test Sistemi

```python
# backend/services/regression_test_service.py

import asyncio
from datetime import datetime
from typing import List, Dict
import json

class RegressionTestService:
    """
    Her deployment Ã¶ncesi otomatik Ã§alÄ±ÅŸan regression testleri.
    Daha Ã¶nce doÄŸru Ã§alÄ±ÅŸan analizlerin bozulup bozulmadÄ±ÄŸÄ±nÄ± kontrol eder.
    """
    
    def __init__(self, ai_service, db_manager):
        self.ai_service = ai_service
        self.db = db_manager
        self.baseline_path = "tests/regression_baseline.json"
        
    async def run_regression_tests(self) -> Dict:
        """TÃ¼m regression testlerini Ã§alÄ±ÅŸtÄ±r"""
        
        # Baseline'Ä± yÃ¼kle
        baseline = self._load_baseline()
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'total': len(baseline['test_cases']),
            'passed': 0,
            'failed': 0,
            'regressions': [],
            'improvements': []
        }
        
        for test_case in baseline['test_cases']:
            # Yeni sonuÃ§ al
            new_result = await self.ai_service.generate_analysis(
                test_case['description'],
                test_case['unit']
            )
            
            # Baseline ile karÅŸÄ±laÅŸtÄ±r
            comparison = self._compare_results(
                test_case['expected_result'],
                new_result,
                test_case['validation_rules']
            )
            
            if comparison['passed']:
                results['passed'] += 1
            else:
                results['failed'] += 1
                results['regressions'].append({
                    'test_id': test_case['id'],
                    'description': test_case['description'],
                    'issues': comparison['issues']
                })
            
            # Ä°yileÅŸme var mÄ±?
            if comparison.get('improved'):
                results['improvements'].append({
                    'test_id': test_case['id'],
                    'improvement': comparison['improvement_detail']
                })
        
        # SonuÃ§larÄ± kaydet
        self._save_results(results)
        
        return results
    
    def _compare_results(self, expected: Dict, actual: Dict, rules: Dict) -> Dict:
        """Ä°ki sonucu karÅŸÄ±laÅŸtÄ±r"""
        
        issues = []
        improved = False
        
        expected_components = expected.get('components', [])
        actual_components = actual.get('components', [])
        
        # 1. Zorunlu bileÅŸenler
        for rule in rules.get('required_components', []):
            found = any(
                rule['keyword'].lower() in c.get('name', '').lower()
                for c in actual_components
            )
            if not found:
                issues.append(f"Zorunlu bileÅŸen eksik: {rule['keyword']}")
        
        # 2. Yasak bileÅŸenler
        for rule in rules.get('forbidden_components', []):
            found = any(
                rule['keyword'].lower() in c.get('name', '').lower()
                for c in actual_components
            )
            if found:
                issues.append(f"Yasak bileÅŸen var: {rule['keyword']}")
        
        # 3. Miktar toleransÄ±
        for exp_comp in expected_components:
            for act_comp in actual_components:
                if self._components_match(exp_comp, act_comp):
                    exp_qty = exp_comp.get('quantity', 0)
                    act_qty = act_comp.get('quantity', 0)
                    
                    if exp_qty > 0:
                        deviation = abs(act_qty - exp_qty) / exp_qty
                        tolerance = rules.get('quantity_tolerance', 0.2)
                        
                        if deviation > tolerance:
                            issues.append(
                                f"Miktar sapmasÄ±: {exp_comp.get('name')} "
                                f"(beklenen: {exp_qty}, gerÃ§ek: {act_qty})"
                            )
        
        return {
            'passed': len(issues) == 0,
            'issues': issues,
            'improved': improved
        }
    
    def update_baseline(self, test_id: str, new_expected: Dict):
        """Baseline'Ä± gÃ¼ncelle (manuel onay sonrasÄ±)"""
        
        baseline = self._load_baseline()
        
        for test_case in baseline['test_cases']:
            if test_case['id'] == test_id:
                test_case['expected_result'] = new_expected
                test_case['updated_at'] = datetime.now().isoformat()
                break
        
        self._save_baseline(baseline)
        print(f"âœ… Baseline gÃ¼ncellendi: {test_id}")
```

---

### 3.2 GerÃ§ek ZamanlÄ± Monitoring Dashboard

```python
# backend/services/monitoring_service.py

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict
import statistics

@dataclass
class AnalysisMetrics:
    """Analiz metrikleri"""
    timestamp: datetime
    description: str
    duration_ms: float
    component_count: int
    total_price: float
    confidence_score: float
    validation_status: str
    user_edited: bool

class MonitoringService:
    """GerÃ§ek zamanlÄ± analiz izleme"""
    
    def __init__(self, db_manager):
        self.db = db_manager
        self.metrics_buffer: List[AnalysisMetrics] = []
        
    def record_analysis(self, metrics: AnalysisMetrics):
        """Analiz metriklerini kaydet"""
        self.metrics_buffer.append(metrics)
        
        # Her 100 kayÄ±tta veya 5 dakikada bir flush
        if len(self.metrics_buffer) >= 100:
            self._flush_to_db()
    
    def get_dashboard_stats(self, hours: int = 24) -> Dict:
        """Dashboard istatistiklerini getir"""
        
        since = datetime.now() - timedelta(hours=hours)
        metrics = self.db.get_metrics_since(since)
        
        if not metrics:
            return {'error': 'Veri yok'}
        
        # Temel istatistikler
        durations = [m.duration_ms for m in metrics]
        prices = [m.total_price for m in metrics]
        confidences = [m.confidence_score for m in metrics]
        
        return {
            'period': f'Son {hours} saat',
            'total_analyses': len(metrics),
            'performance': {
                'avg_duration_ms': statistics.mean(durations),
                'p95_duration_ms': self._percentile(durations, 95),
                'p99_duration_ms': self._percentile(durations, 99)
            },
            'quality': {
                'avg_confidence': statistics.mean(confidences),
                'validation_pass_rate': sum(1 for m in metrics if m.validation_status == 'ok') / len(metrics),
                'user_edit_rate': sum(1 for m in metrics if m.user_edited) / len(metrics)
            },
            'pricing': {
                'avg_total_price': statistics.mean(prices),
                'min_price': min(prices),
                'max_price': max(prices)
            },
            'trends': self._calculate_trends(metrics)
        }
    
    def _calculate_trends(self, metrics: List[AnalysisMetrics]) -> Dict:
        """Trend analizi"""
        
        # Son 6 saati 1 saatlik dilimlere bÃ¶l
        hourly_buckets = {}
        
        for m in metrics:
            hour_key = m.timestamp.strftime('%Y-%m-%d %H:00')
            if hour_key not in hourly_buckets:
                hourly_buckets[hour_key] = []
            hourly_buckets[hour_key].append(m)
        
        trends = []
        for hour, bucket_metrics in sorted(hourly_buckets.items()):
            trends.append({
                'hour': hour,
                'count': len(bucket_metrics),
                'avg_confidence': statistics.mean(m.confidence_score for m in bucket_metrics),
                'edit_rate': sum(1 for m in bucket_metrics if m.user_edited) / len(bucket_metrics)
            })
        
        return trends
    
    def get_alerts(self) -> List[Dict]:
        """Aktif uyarÄ±larÄ± getir"""
        
        alerts = []
        stats = self.get_dashboard_stats(hours=1)  # Son 1 saat
        
        # Performans uyarÄ±sÄ±
        if stats['performance']['p95_duration_ms'] > 5000:
            alerts.append({
                'level': 'warning',
                'type': 'performance',
                'message': f"P95 yanÄ±t sÃ¼resi yÃ¼ksek: {stats['performance']['p95_duration_ms']:.0f}ms"
            })
        
        # Kalite uyarÄ±sÄ±
        if stats['quality']['user_edit_rate'] > 0.3:
            alerts.append({
                'level': 'warning',
                'type': 'quality',
                'message': f"KullanÄ±cÄ± dÃ¼zeltme oranÄ± yÃ¼ksek: {stats['quality']['user_edit_rate']:.0%}"
            })
        
        # Validasyon uyarÄ±sÄ±
        if stats['quality']['validation_pass_rate'] < 0.8:
            alerts.append({
                'level': 'critical',
                'type': 'validation',
                'message': f"Validasyon geÃ§me oranÄ± dÃ¼ÅŸÃ¼k: {stats['quality']['validation_pass_rate']:.0%}"
            })
        
        return alerts
```

---

### 3.3 A/B Test Sistemi

```python
# backend/services/ab_test_service.py

import random
from typing import Dict, Optional
from datetime import datetime

class ABTestService:
    """
    FarklÄ± prompt/model konfigÃ¼rasyonlarÄ±nÄ± A/B test et.
    Hangi yaklaÅŸÄ±mÄ±n daha iyi sonuÃ§ verdiÄŸini Ã¶lÃ§.
    """
    
    def __init__(self, db_manager):
        self.db = db_manager
        self.active_experiments = {}
        
    def create_experiment(
        self,
        name: str,
        variants: List[Dict],
        traffic_split: List[float] = None
    ) -> str:
        """Yeni A/B testi oluÅŸtur"""
        
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        if traffic_split is None:
            # EÅŸit daÄŸÄ±lÄ±m
            traffic_split = [1.0 / len(variants)] * len(variants)
        
        experiment = {
            'id': experiment_id,
            'name': name,
            'variants': variants,
            'traffic_split': traffic_split,
            'created_at': datetime.now(),
            'status': 'active',
            'results': {v['name']: {'count': 0, 'edits': 0, 'confidence_sum': 0} for v in variants}
        }
        
        self.active_experiments[experiment_id] = experiment
        self.db.save_experiment(experiment)
        
        return experiment_id
    
    def get_variant(self, experiment_id: str, user_id: str = None) -> Dict:
        """KullanÄ±cÄ± iÃ§in variant seÃ§"""
        
        experiment = self.active_experiments.get(experiment_id)
        if not experiment or experiment['status'] != 'active':
            return None
        
        # Deterministic assignment (aynÄ± kullanÄ±cÄ± hep aynÄ± variant'Ä± gÃ¶rÃ¼r)
        if user_id:
            hash_val = hash(f"{experiment_id}_{user_id}") % 100 / 100
        else:
            hash_val = random.random()
        
        cumulative = 0
        for variant, split in zip(experiment['variants'], experiment['traffic_split']):
            cumulative += split
            if hash_val < cumulative:
                return variant
        
        return experiment['variants'][-1]
    
    def record_result(
        self,
        experiment_id: str,
        variant_name: str,
        user_edited: bool,
        confidence_score: float
    ):
        """Test sonucunu kaydet"""
        
        experiment = self.active_experiments.get(experiment_id)
        if not experiment:
            return
        
        results = experiment['results'][variant_name]
        results['count'] += 1
        results['confidence_sum'] += confidence_score
        
        if user_edited:
            results['edits'] += 1
        
        # Her 100 kayÄ±tta DB'ye yaz
        if sum(r['count'] for r in experiment['results'].values()) % 100 == 0:
            self.db.update_experiment(experiment)
    
    def get_experiment_results(self, experiment_id: str) -> Dict:
        """Deney sonuÃ§larÄ±nÄ± getir"""
        
        experiment = self.active_experiments.get(experiment_id)
        if not experiment:
            return None
        
        analysis = {}
        
        for variant_name, results in experiment['results'].items():
            count = results['count']
            if count == 0:
                continue
            
            edit_rate = results['edits'] / count
            avg_confidence = results['confidence_sum'] / count
            
            analysis[variant_name] = {
                'sample_size': count,
                'edit_rate': edit_rate,
                'avg_confidence': avg_confidence,
                'score': avg_confidence * (1 - edit_rate)  # Composite score
            }
        
        # Ä°statistiksel anlamlÄ±lÄ±k (basit z-test)
        if len(analysis) >= 2:
            variants = list(analysis.keys())
            v1, v2 = variants[0], variants[1]
            
            n1, n2 = analysis[v1]['sample_size'], analysis[v2]['sample_size']
            p1, p2 = analysis[v1]['edit_rate'], analysis[v2]['edit_rate']
            
            # Pooled proportion
            p_pool = (analysis[v1]['edit_rate'] * n1 + analysis[v2]['edit_rate'] * n2) / (n1 + n2)
            
            # Standard error
            se = (p_pool * (1 - p_pool) * (1/n1 + 1/n2)) ** 0.5
            
            # Z-score
            if se > 0:
                z_score = (p1 - p2) / se
                significant = abs(z_score) > 1.96  # 95% confidence
            else:
                z_score = 0
                significant = False
            
            analysis['_statistics'] = {
                'z_score': z_score,
                'significant': significant,
                'winner': v1 if analysis[v1]['score'] > analysis[v2]['score'] else v2
            }
        
        return {
            'experiment_id': experiment_id,
            'name': experiment['name'],
            'status': experiment['status'],
            'analysis': analysis
        }


# KullanÄ±m Ã¶rneÄŸi
"""
# 1. Deney oluÅŸtur
ab_service.create_experiment(
    name="CoT vs Standard Prompt",
    variants=[
        {'name': 'control', 'prompt_type': 'standard'},
        {'name': 'treatment', 'prompt_type': 'chain_of_thought'}
    ]
)

# 2. Her istekte variant seÃ§
variant = ab_service.get_variant(experiment_id, user_id)

# 3. Variant'a gÃ¶re analiz yap
if variant['prompt_type'] == 'chain_of_thought':
    result = cot_service.analyze(description, unit)
else:
    result = standard_service.analyze(description, unit)

# 4. Sonucu kaydet
ab_service.record_result(
    experiment_id,
    variant['name'],
    user_edited=was_edited,
    confidence_score=result['confidence']
)
"""
```

---

## ğŸ“Š BÃ–LÃœM 4: KULLANICI DENEYÄ°MÄ° Ä°YÄ°LEÅTÄ°RMELERÄ°

### 4.1 AkÄ±llÄ± Otomatik Tamamlama

```python
# backend/services/autocomplete_service.py

from typing import List, Dict
import re

class SmartAutocompleteService:
    """
    KullanÄ±cÄ± yazarken akÄ±llÄ± Ã¶neriler sun.
    GeÃ§miÅŸ aramalar + POZ veritabanÄ± + popÃ¼ler kombinasyonlar
    """
    
    def __init__(self, db_manager, vector_service):
        self.db = db_manager
        self.vector = vector_service
        
        # PopÃ¼ler arama kalÄ±plarÄ±
        self.popular_patterns = [
            "C{class} {type}",  # C25/30 beton
            "{thickness} cm {material}",  # 20 cm tuÄŸla
            "Ã˜{diameter} {steel}",  # Ã˜14 demir
        ]
        
    def get_suggestions(self, partial_text: str, limit: int = 10) -> List[Dict]:
        """YazÄ±lan metne gÃ¶re Ã¶neriler getir"""
        
        suggestions = []
        
        # 1. GeÃ§miÅŸ baÅŸarÄ±lÄ± aramalar
        history_suggestions = self.db.get_successful_searches(
            prefix=partial_text,
            limit=5
        )
        suggestions.extend([{
            'text': s['description'],
            'type': 'history',
            'frequency': s['count']
        } for s in history_suggestions])
        
        # 2. POZ veritabanÄ±ndan eÅŸleÅŸmeler
        poz_suggestions = self.vector.search(partial_text, top_k=5)
        suggestions.extend([{
            'text': s['description'],
            'type': 'poz',
            'poz_no': s['poz_no'],
            'similarity': s.get('similarity_score', 0)
        } for s in poz_suggestions])
        
        # 3. AkÄ±llÄ± tamamlamalar
        smart_completions = self._generate_smart_completions(partial_text)
        suggestions.extend(smart_completions)
        
        # SÄ±rala ve limitle
        suggestions = sorted(
            suggestions,
            key=lambda x: x.get('frequency', 0) + x.get('similarity', 0),
            reverse=True
        )[:limit]
        
        return suggestions
    
    def _generate_smart_completions(self, text: str) -> List[Dict]:
        """AkÄ±llÄ± tamamlama Ã¶nerileri"""
        
        completions = []
        text_lower = text.lower()
        
        # Beton sÄ±nÄ±fÄ± tamamlama
        if 'beton' in text_lower and not re.search(r'c\d+', text_lower):
            for concrete_class in ['C20/25', 'C25/30', 'C30/37', 'C35/45']:
                completions.append({
                    'text': f"{text} {concrete_class}",
                    'type': 'smart',
                    'reason': 'Beton sÄ±nÄ±fÄ± Ã¶nerisi'
                })
        
        # Duvar kalÄ±nlÄ±ÄŸÄ± tamamlama
        if any(w in text_lower for w in ['duvar', 'tuÄŸla', 'briket']):
            if not re.search(r'\d+\s*cm', text_lower):
                for thickness in ['10 cm', '15 cm', '20 cm']:
                    completions.append({
                        'text': f"{thickness} {text}",
                        'type': 'smart',
                        'reason': 'KalÄ±nlÄ±k Ã¶nerisi'
                    })
        
        # Demir Ã§apÄ± tamamlama
        if 'demir' in text_lower and not re.search(r'Ã¸\d+', text_lower):
            for diameter in ['Ã˜12', 'Ã˜14', 'Ã˜16']:
                completions.append({
                    'text': f"{text} {diameter}",
                    'type': 'smart',
                    'reason': 'Ã‡ap Ã¶nerisi'
                })
        
        return completions[:3]
```

---

### 4.2 GÃ¶rsel Analiz KarÅŸÄ±laÅŸtÄ±rma

```typescript
// web-app/app/components/AnalysisComparison.tsx

import React from 'react';
import { Diff } from 'react-diff-viewer-continued';

interface Component {
  type: string;
  name: string;
  quantity: number;
  unit_price: number;
}

interface AnalysisComparisonProps {
  originalAnalysis: Component[];
  correctedAnalysis: Component[];
  onSaveFeedback: (diff: any) => void;
}

export const AnalysisComparison: React.FC<AnalysisComparisonProps> = ({
  originalAnalysis,
  correctedAnalysis,
  onSaveFeedback
}) => {
  // DeÄŸiÅŸiklikleri hesapla
  const changes = calculateChanges(originalAnalysis, correctedAnalysis);
  
  return (
    <div className="analysis-comparison">
      <h3>ğŸ“Š Analiz KarÅŸÄ±laÅŸtÄ±rma</h3>
      
      {/* Ã–zet */}
      <div className="changes-summary">
        <div className="stat added">
          <span className="icon">â•</span>
          <span className="count">{changes.added.length}</span>
          <span className="label">Eklenen</span>
        </div>
        <div className="stat removed">
          <span className="icon">â–</span>
          <span className="count">{changes.removed.length}</span>
          <span className="label">Ã‡Ä±karÄ±lan</span>
        </div>
        <div className="stat modified">
          <span className="icon">âœï¸</span>
          <span className="count">{changes.modified.length}</span>
          <span className="label">DeÄŸiÅŸtirilen</span>
        </div>
      </div>
      
      {/* DetaylÄ± DeÄŸiÅŸiklikler */}
      <div className="changes-detail">
        {changes.added.map((item, i) => (
          <div key={`added-${i}`} className="change-item added">
            <span className="badge">â• Eklendi</span>
            <span className="name">{item.name}</span>
            <span className="quantity">{item.quantity} {item.unit}</span>
          </div>
        ))}
        
        {changes.removed.map((item, i) => (
          <div key={`removed-${i}`} className="change-item removed">
            <span className="badge">â– Ã‡Ä±karÄ±ldÄ±</span>
            <span className="name">{item.name}</span>
            <span className="reason">
              <input 
                type="text" 
                placeholder="Neden Ã§Ä±karÄ±ldÄ±?"
                onChange={(e) => item.reason = e.target.value}
              />
            </span>
          </div>
        ))}
        
        {changes.modified.map((item, i) => (
          <div key={`modified-${i}`} className="change-item modified">
            <span className="badge">âœï¸ DeÄŸiÅŸtirildi</span>
            <span className="name">{item.name}</span>
            <div className="diff">
              <span className="old">{item.oldValue}</span>
              <span className="arrow">â†’</span>
              <span className="new">{item.newValue}</span>
            </div>
          </div>
        ))}
      </div>
      
      {/* Fiyat Etkisi */}
      <div className="price-impact">
        <h4>ğŸ’° Fiyat Etkisi</h4>
        <div className="price-diff">
          <span className="label">Orijinal:</span>
          <span className="value">{formatPrice(changes.originalTotal)} TL</span>
        </div>
        <div className="price-diff">
          <span className="label">DÃ¼zeltilmiÅŸ:</span>
          <span className="value">{formatPrice(changes.correctedTotal)} TL</span>
        </div>
        <div className="price-diff highlight">
          <span className="label">Fark:</span>
          <span className={`value ${changes.priceDiff > 0 ? 'positive' : 'negative'}`}>
            {changes.priceDiff > 0 ? '+' : ''}{formatPrice(changes.priceDiff)} TL
            ({((changes.priceDiff / changes.originalTotal) * 100).toFixed(1)}%)
          </span>
        </div>
      </div>
      
      {/* Kaydet */}
      <button 
        className="save-feedback-btn"
        onClick={() => onSaveFeedback(changes)}
      >
        ğŸ“ DÃ¼zeltmeyi AI'ya Ã–ÄŸret
      </button>
    </div>
  );
};
```

---

## ğŸ“Š BÃ–LÃœM 5: Ã–LÃ‡EKLENEBÄ°LÄ°RLÄ°K

### 5.1 Caching Stratejisi

```python
# backend/services/caching_service.py

import hashlib
import json
from datetime import datetime, timedelta
from typing import Optional, Dict
import redis

class AnalysisCacheService:
    """
    Analiz sonuÃ§larÄ±nÄ± cache'le.
    AynÄ±/benzer sorgular iÃ§in tekrar AI Ã§aÄŸrÄ±sÄ± yapma.
    """
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl = timedelta(hours=24)  # 24 saat cache
        
    def _generate_cache_key(self, description: str, unit: str) -> str:
        """NormalleÅŸtirilmiÅŸ cache key oluÅŸtur"""
        
        # Normalize: lowercase, trim, sort words
        normalized = ' '.join(sorted(description.lower().split()))
        
        # Hash
        content = f"{normalized}|{unit.lower()}"
        return f"analysis:{hashlib.md5(content.encode()).hexdigest()}"
    
    def get_cached(self, description: str, unit: str) -> Optional[Dict]:
        """Cache'den sonuÃ§ getir"""
        
        key = self._generate_cache_key(description, unit)
        cached = self.redis.get(key)
        
        if cached:
            data = json.loads(cached)
            data['_from_cache'] = True
            return data
        
        return None
    
    def set_cached(self, description: str, unit: str, result: Dict):
        """Sonucu cache'e kaydet"""
        
        key = self._generate_cache_key(description, unit)
        
        # Sadece baÅŸarÄ±lÄ± sonuÃ§larÄ± cache'le
        if result.get('components'):
            self.redis.setex(
                key,
                self.ttl,
                json.dumps(result, ensure_ascii=False)
            )
    
    def invalidate_similar(self, description: str):
        """Benzer sorgularÄ±n cache'ini temizle"""
        
        # Bu basit implementasyon sadece exact match temizler
        # Daha geliÅŸmiÅŸ versiyon semantic similarity kullanabilir
        pattern = f"analysis:*"
        for key in self.redis.scan_iter(match=pattern):
            self.redis.delete(key)


class SmartCacheService(AnalysisCacheService):
    """
    Semantik benzerlik tabanlÄ± akÄ±llÄ± cache.
    Tam eÅŸleÅŸme olmasa bile benzer sorgular iÃ§in cache kullan.
    """
    
    def __init__(self, redis_url: str, vector_service):
        super().__init__(redis_url)
        self.vector = vector_service
        self.similarity_threshold = 0.95
        
    def get_cached_semantic(self, description: str, unit: str) -> Optional[Dict]:
        """Semantik benzerlik ile cache ara"""
        
        # 1. Exact match dene
        exact = self.get_cached(description, unit)
        if exact:
            return exact
        
        # 2. Semantik arama
        similar_keys = self._find_similar_cached(description)
        
        for key, similarity in similar_keys:
            if similarity >= self.similarity_threshold:
                cached = self.redis.get(key)
                if cached:
                    data = json.loads(cached)
                    data['_from_cache'] = True
                    data['_cache_similarity'] = similarity
                    return data
        
        return None
```

---

## ğŸ“‹ Ã–ZET: PRÄ°ORÄ°TE SIRASI

| Ã–ncelik | Ä°yileÅŸtirme | Etki | Zorluk | ROI |
|---------|-------------|------|--------|-----|
| ğŸ”´ 1 | RAG Sistemi | Ã‡ok YÃ¼ksek | Orta | â­â­â­â­â­ |
| ğŸ”´ 2 | Chain-of-Thought Prompting | YÃ¼ksek | DÃ¼ÅŸÃ¼k | â­â­â­â­â­ |
| ğŸ”´ 3 | Active Learning | YÃ¼ksek | Orta | â­â­â­â­ |
| ğŸŸ¡ 4 | Regression Test Sistemi | Orta | DÃ¼ÅŸÃ¼k | â­â­â­â­ |
| ğŸŸ¡ 5 | A/B Test Sistemi | Orta | Orta | â­â­â­â­ |
| ğŸŸ¡ 6 | Monitoring Dashboard | Orta | Orta | â­â­â­ |
| ğŸŸ¢ 7 | Fine-Tuning | Ã‡ok YÃ¼ksek | YÃ¼ksek | â­â­â­ |
| ğŸŸ¢ 8 | Caching | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | â­â­â­ |
| ğŸ”µ 9 | AkÄ±llÄ± Autocomplete | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | â­â­ |
| ğŸ”µ 10 | Sentetik Veri | Orta | Orta | â­â­ |

---

## ğŸ¯ Hemen BaÅŸlayabileceÄŸin 3 Åey

1. **Chain-of-Thought ekle** (30 dakika)
   - Mevcut prompt'a `<thinking>` bÃ¶lÃ¼mÃ¼ ekle
   - AI'Ä±n adÄ±m adÄ±m dÃ¼ÅŸÃ¼nmesini saÄŸla

2. **Regression test baseline oluÅŸtur** (1 saat)
   - Mevcut golden dataset'i geniÅŸlet
   - Her PR Ã¶ncesi otomatik Ã§alÄ±ÅŸacak test yaz

3. **Monitoring metriklerini kaydet** (2 saat)
   - Her analiz iÃ§in sÃ¼re, gÃ¼ven skoru, dÃ¼zeltme durumu kaydet
   - Basit dashboard oluÅŸtur

---

**HazÄ±rlayan:** Claude AI  
**Tarih:** 2026-02-02
